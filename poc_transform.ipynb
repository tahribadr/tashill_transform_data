{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules here are updated everytime I run a cell, except those excluded by %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data_transformation_funcs as dt\n",
    "import data_specific_funcs as ds\n",
    "\n",
    "%aimport random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell is where you have to provide original data information to be transformed, specifically:\n",
    "\n",
    "### mandatory information: \n",
    "\n",
    "- **original_train_list**: a python list of train sequences, each sequence is a python list of strings\n",
    "- **original_test_list**: same format as original_train_list but with test sequences\n",
    "- **original_test_labels**: a python list of the test labels, each test label is a string\n",
    "- **task** : a string that describes the task learned from the dataset, can be either \"lm\" for language model or \"classif\" for classification (binary or multiple, both are accepted)\n",
    "- **categorical**: a boolean that is True if the labels on the dataset are categorical, false if they are numerical.\n",
    "\n",
    "### conditional information:\n",
    "if the task on the dataset is \"classif\", you will also have to provide:\n",
    "\n",
    "- **original_train_labels** : a python list of the train labels, each label is a string\n",
    "\n",
    "### optional information:\n",
    "these lists are not mandatory, if you do not provide them, the function will automatically carve them out of the \n",
    "training set while respecting the valid_ratio property.\n",
    "\n",
    "- **original_valid_list**: a python list of validation/dev sequences, each sequence is a python list of strings\n",
    "- **original_valid_labels**: only if task=\"classif\", a python list of the validation/dev labels, each label is a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for langage modeling task with non categorical data and no provided validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 ['5', '4', '1', '1', '5', '3', '4', '7', '4', '7', '5', '0']\n"
     ]
    }
   ],
   "source": [
    "# open text file and read in data \n",
    "data_path = 'PAutomaC-competition_sets'\n",
    "paut_id = '1'\n",
    "\n",
    "with open(data_path+'/'+paut_id+'.pautomac.train', 'r') as f:\n",
    "    original_train_text = f.read()\n",
    "with open(data_path+'/'+paut_id+'.pautomac.test', 'r') as f:\n",
    "    original_test_text = f.read()\n",
    "with open(data_path+'/'+paut_id+'.pautomac_solution.txt', 'r') as f:\n",
    "    original_solution_text = f.read()\n",
    "    \n",
    "#extract list of sequences, each sequence being a list of strings, and list of labels, each label being a string.\n",
    "original_train_list = ds.extract_examples_pautomac(original_train_text)\n",
    "original_test_list = ds.extract_examples_pautomac(original_test_text)\n",
    "original_test_labels = ds.extract_labels_pautomac(original_solution_text)\n",
    "\n",
    "task_paut = \"lm\" # task can be either \"lm\" or \"classif\"\n",
    "categorical_paut = False #pautomac labels are scores, not categorical values\n",
    "\n",
    "print(len(original_train_list), original_train_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18180 [8, 7, 3, 0, 1, 9]\n",
      "1818 [8, 3, 2, 1, 7, 5, 6, 3, 0, 7, 5, 1, 7, 4, 3, 3, 4, 3, 5, 3, 3, 2, 4, 3, 5, 1, 3, 6, 6, 3, 9]\n",
      "1000 [8, 1, 0, 5, 7, 2, 7, 5, 2, 4, 9]\n",
      "1000 7.43792566036e-09\n"
     ]
    }
   ],
   "source": [
    "#using a seed to get the same consistent random mapping everytime \n",
    "#Note: if working with a notebook, this has to be on the same cell as the function that calls random\n",
    "random.seed(4242)\n",
    "\n",
    "# we generate the transformed lists\n",
    "train_list, valid_list, test_list, test_labels, dataset_key = dt.transform_lists(original_train_list,\n",
    "                                                                                 original_test_list,\n",
    "                                                                                 original_test_labels,\n",
    "                                                                                 categorical = categorical_paut,\n",
    "                                                                                 task=task_paut)\n",
    "\n",
    "print(len(train_list), train_list[0])\n",
    "print(len(valid_list), valid_list[0])\n",
    "print(len(test_list), test_list[0])\n",
    "print(len(test_labels), test_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we output these lists as txt documents in the pautomac format\n",
    "# if no target folder is given, the datasets will be output in the current directory\n",
    "\n",
    "\n",
    "tashill_id = 1 # this number determines the prefix of the file, naming of files also respects pautomac standard\n",
    "\n",
    "dt.make_competition_sets(train_list, \n",
    "                            valid_list, \n",
    "                            test_list, \n",
    "                            test_labels,\n",
    "                            data_id = tashill_id,\n",
    "                            categorical = categorical_paut,\n",
    "                            target_path = \"tashill_sets\",\n",
    "                            task = task_paut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we generated an object called \"dataset_key\" in the cells before. This object serves to decode and retrieve the original dataset. It should not be given to participants.\n",
    "\n",
    "In the next cell, we will use it to recover the original dataset. This function is not perfectly bijective because we carved out a validation set from the training set, and we also got rid of a few examples to allow for a perfect valid_ratio.\n",
    "\n",
    "However if you see the generated file in the \"pautomac_competition_sets\" folder, you will notice we perfectly recover the first 18180 training sequences for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18180 ['5', '4', '1', '1', '5', '3', '4', '7', '4', '7', '5', '0']\n"
     ]
    }
   ],
   "source": [
    "recovered_train_list, recovered_valid_list, recovered_list, recovered_test_labels = dt.reverse_transform(train_list, \n",
    "                                                                        valid_list, \n",
    "                                                                        test_list, \n",
    "                                                                        test_labels, \n",
    "                                                                        dataset_key,\n",
    "                                                                        categorical = categorical_paut,\n",
    "                                                                        task = task_paut)\n",
    "print(len(recovered_train_list), recovered_train_list[0])\n",
    "with open(data_path+'/'+paut_id+'.recovered_pautomac.train', 'w') as f:\n",
    "     f.write(dt.generate_data_text(recovered_train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for binary classification task with categorical data and a provided validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511538 ['M', 'A', 'D', 'P', 'S', 'L', 'Y', 'T', 'Y', 'P', 'S', 'P', 'L', 'Q', 'G', 'Y', 'E', 'N', 'L', 'A', 'P', 'L', 'G', 'T', 'E', 'V', 'S', 'P', 'D', 'G', 'K', 'S', 'L', 'L', 'N', 'P', 'E', 'T', 'G', 'I', 'K', 'S', 'K', 'S', 'Y', 'E', 'K', 'F', 'T', 'E', 'P', 'L', 'D', 'S', 'G', 'I', 'R', 'G', 'A', 'F', 'D', 'V', 'H', 'I', 'Y', 'H', 'F', 'Q', 'K', 'N', 'K', 'E', 'Q', 'A', 'K', 'F', 'A', 'R', 'E', 'L', 'W', 'E', 'R', 'I', 'R', 'R', 'E', 'F', 'P', 'E', 'L', 'R', 'I', 'Y', 'R', 'F', 'W', 'E', 'E', 'P', 'I', 'G', 'P', 'H', 'P', 'V', 'A', 'M', 'F', 'E', 'V', 'N', 'L', 'F', 'T', 'P', 'E', 'Q', 'F', 'G', 'A', 'F', 'I', 'P', 'W', 'L', 'V', 'I', 'N', 'R', 'G', 'P', 'L', 'S', 'A', 'L', 'V', 'H', 'P', 'N', 'T', 'V', 'D', 'E', 'K', 'G', 'E', 'L', 'L', 'D', 'E', 'E', 'R', 'D', 'H', 'T', 'Q', 'R', 'A', 'I', 'W', 'M', 'G', 'E', 'Q', 'L', 'P', 'L', 'D', 'L', 'S', 'L', 'V', 'K', 'R', 'L', 'K', 'Q', 'Q', 'K', 'A', 'A', 'H'] \n",
      " 511538 ['FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open text file and read in data \n",
    "#heinz_data_path = 'HeinzData'\n",
    "#heinz_data_id = '16.16.LT.4.1.5'\n",
    "heinz_data_path = 'ENE_sets'\n",
    "heinz_data_id = 'ENE'\n",
    "\n",
    "#with open(heinz_data_path+'/'+heinz_data_id+'_Train.txt', 'r') as f:\n",
    "#    train_text = f.read()\n",
    "#with open(heinz_data_path+'/'+heinz_data_id+'_Dev.txt', 'r') as f:\n",
    "#    valid_text = f.read()\n",
    "#with open(heinz_data_path+'/'+heinz_data_id+'_TestSR.txt', 'r') as f:\n",
    "#    testSR_text = f.read()\n",
    "\n",
    "with open(heinz_data_path+'/'+heinz_data_id+'.train', 'r') as f:\n",
    "    train_text = f.read()\n",
    "with open(heinz_data_path+'/'+heinz_data_id+'.valid', 'r') as f:\n",
    "    valid_text = f.read()\n",
    "with open(heinz_data_path+'/'+heinz_data_id+'.test', 'r') as f:\n",
    "    testSR_text = f.read()\n",
    "\n",
    "#extract list of sequences, each sequence being a list of strings, and list of labels, each label being a string.\n",
    "original_train_list, original_train_label = ds.extract_HeinzData(train_text)\n",
    "original_valid_list, original_valid_label = ds.extract_HeinzData(valid_text)\n",
    "original_test_list, original_test_label = ds.extract_HeinzData(testSR_text)\n",
    "\n",
    "task_heinz = \"classif\"\n",
    "categorical_heinz = True\n",
    "\n",
    "print(len(original_train_list), original_train_list[0],\"\\n\",\n",
    "      len(original_train_label), original_train_label[:10],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set: \n",
      " ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] 23\n",
      "train set: \n",
      " ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] 25\n",
      "test set: \n",
      " ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] 24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = dt.generate_vocab(original_valid_list)\n",
    "vocab_size = len( vocab )\n",
    "print(\"validation set: \\n\",vocab, vocab_size)\n",
    "\n",
    "vocab = dt.generate_vocab(original_train_list)\n",
    "vocab_size = len( vocab )\n",
    "print(\"train set: \\n\",vocab, vocab_size)\n",
    "\n",
    "vocab = dt.generate_vocab(original_test_list)\n",
    "vocab_size = len( vocab )\n",
    "print(\"test set: \\n\",vocab, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47750 [25, 24, 1, 12, 9, 12, 23, 22, 9, 15, 7, 7, 1, 11, 12, 10, 15, 12, 19, 21, 5, 5, 10, 15, 8, 23, 20, 10, 7, 22, 3, 19, 5, 19, 13, 12, 21, 22, 7, 19, 15, 12, 20, 22, 12, 15, 22, 23, 22, 12, 13, 23, 7, 5, 13, 8, 24, 15, 22, 23, 12, 5, 15, 13, 16, 1, 20, 10, 10, 15, 15, 10, 10, 23, 1, 23, 5, 13, 22, 23, 12, 12, 1, 8, 5, 13, 22, 8, 14, 16, 7, 12, 3, 7, 3, 13, 14, 22, 19, 23, 3, 14, 0, 10, 12, 19, 1, 19, 11, 9, 1, 13, 20, 13, 8, 5, 23, 7, 13, 22, 7, 11, 15, 21, 3, 9, 24, 20, 3, 13, 19, 21, 1, 1, 9, 26]\n",
      "4775 [25, 24, 9, 20, 14, 22, 13, 13, 13, 13, 13, 22, 20, 10, 13, 1, 13, 22, 13, 13, 11, 13, 1, 11, 10, 1, 14, 10, 13, 11, 11, 19, 11, 7, 24, 3, 12, 7, 5, 19, 5, 13, 19, 20, 7, 23, 21, 15, 10, 12, 19, 10, 14, 5, 15, 3, 23, 7, 20, 15, 22, 1, 10, 19, 19, 13, 19, 10, 22, 22, 0, 12, 13, 9, 7, 11, 10, 22, 11, 1, 7, 23, 5, 14, 0, 10, 12, 1, 15, 3, 21, 20, 13, 22, 1, 1, 10, 15, 9, 3, 12, 20, 13, 22, 1, 22, 3, 8, 12, 22, 10, 22, 13, 23, 10, 20, 3, 10, 14, 10, 12, 5, 20, 14, 8, 0, 21, 13, 10, 10, 22, 10, 19, 20, 5, 0, 9, 9, 9, 7, 11, 14, 14, 5, 23, 5, 19, 1, 22, 11, 19, 13, 11, 13, 12, 13, 26]\n",
      "4774 [25, 24, 1, 14, 0, 7, 3, 1, 11, 15, 12, 15, 11, 23, 19, 23, 7, 7, 23, 1, 12, 15, 1, 13, 13, 23, 7, 7, 23, 11, 12, 15, 11, 13, 5, 23, 7, 7, 14, 1, 12, 15, 13, 23, 19, 23, 7, 7, 23, 1, 12, 15, 11, 23, 19, 23, 7, 7, 23, 1, 12, 15, 1, 13, 13, 23, 7, 7, 13, 22, 12, 15, 11, 13, 10, 13, 7, 7, 5, 11, 12, 15, 1, 13, 10, 21, 24, 14, 1, 1, 12, 16, 5, 13, 12, 3, 16, 15, 10, 5, 15, 21, 10, 7, 19, 23, 11, 0, 0, 0, 11, 21, 15, 24, 14, 1, 13, 19, 12, 15, 20, 11, 23, 20, 0, 15, 10, 3, 22, 24, 21, 1, 24, 8, 15, 19, 19, 26]\n",
      "4774 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#using a seed to get the same consistent random mapping everytime \n",
    "#Note: if working with a notebook, this has to be on the same cell as the function that calls random\n",
    "random.seed(4242)\n",
    "\n",
    "# we generate the transformed lists\n",
    "train_list, train_labels, valid_list, valid_labels, test_list, test_labels, dataset_key = dt.transform_lists(\n",
    "                                                                                            original_train_list,\n",
    "                                                                                            original_test_list,\n",
    "                                                                                            original_test_label,\n",
    "                                                                                            valid_list = original_valid_list, \n",
    "                                                                                            train_labels = original_train_label,\n",
    "                                                                                            valid_labels = original_valid_label,\n",
    "                                                                                            categorical = categorical_heinz,\n",
    "                                                                                            task=task_heinz)\n",
    "\n",
    "print(len(train_list), train_list[0])\n",
    "print(len(valid_list), valid_list[0])\n",
    "print(len(test_list), test_list[0])\n",
    "print(len(test_labels), test_labels[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 13, 'B': 4, 'C': 0, 'D': 19, 'E': 12, 'F': 20, 'G': 10, 'H': 8, 'I': 5, 'K': 15, 'L': 22, 'M': 24, 'N': 7, 'O': 2, 'P': 11, 'Q': 9, 'R': 21, 'S': 1, 'T': 14, 'U': 17, 'V': 23, 'W': 16, 'X': 6, 'Y': 3, 'Z': 18}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_key[\"word_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we output these lists as txt documents in the pautomac format\n",
    "# if no target folder is given, the datasets will be output in the current directory\n",
    "tashill_id = 3\n",
    "dt.make_competition_sets(train_list, \n",
    "                            valid_list, \n",
    "                            test_list, \n",
    "                            test_labels,\n",
    "                            train_labels = train_labels,\n",
    "                            valid_labels = valid_labels,\n",
    "                            data_id = tashill_id,\n",
    "                            categorical = categorical_heinz,\n",
    "                            target_path = \"tashill_sets\",\n",
    "                            task = task_heinz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we recover original sequences as python lists of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(recovered_train_list, \n",
    " recovered_train_labels, \n",
    " recovered_valid_list,\n",
    " recovered_valid_labels,\n",
    " recovered_test_list, \n",
    " recovered_test_labels) = dt.reverse_transform(train_list, \n",
    "                                             valid_list, \n",
    "                                             test_list, \n",
    "                                             test_labels, \n",
    "                                             dataset_key,\n",
    "                                             train_labels = train_labels,\n",
    "                                             valid_labels = valid_labels,\n",
    "                                             categorical = categorical_heinz,\n",
    "                                             task = task_heinz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we output these recovered sequences in pautomac format in the \"Heinzdata\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47750 ['M', 'A', 'D', 'P', 'S', 'L', 'Y', 'T', 'Y', 'P', 'S', 'P', 'L', 'Q', 'G', 'Y', 'E', 'N', 'L', 'A', 'P', 'L', 'G', 'T', 'E', 'V', 'S', 'P', 'D', 'G', 'K', 'S', 'L', 'L', 'N', 'P', 'E', 'T', 'G', 'I', 'K', 'S', 'K', 'S', 'Y', 'E', 'K', 'F', 'T', 'E', 'P', 'L', 'D', 'S', 'G', 'I', 'R', 'G', 'A', 'F', 'D', 'V', 'H', 'I', 'Y', 'H', 'F', 'Q', 'K', 'N', 'K', 'E', 'Q', 'A', 'K', 'F', 'A', 'R', 'E', 'L', 'W', 'E', 'R', 'I', 'R', 'R', 'E', 'F', 'P', 'E', 'L', 'R', 'I', 'Y', 'R', 'F', 'W', 'E', 'E', 'P', 'I', 'G', 'P', 'H', 'P', 'V', 'A', 'M', 'F', 'E', 'V', 'N', 'L', 'F', 'T', 'P', 'E', 'Q', 'F', 'G', 'A', 'F', 'I', 'P', 'W', 'L', 'V', 'I', 'N', 'R', 'G', 'P', 'L', 'S', 'A', 'L', 'V', 'H', 'P', 'N', 'T', 'V', 'D', 'E', 'K', 'G', 'E', 'L', 'L', 'D', 'E', 'E', 'R', 'D', 'H', 'T', 'Q', 'R', 'A', 'I', 'W', 'M', 'G', 'E', 'Q', 'L', 'P', 'L', 'D', 'L', 'S', 'L', 'V', 'K', 'R', 'L', 'K', 'Q', 'Q', 'K', 'A', 'A', 'H']\n",
      "47750 ['FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE']\n"
     ]
    }
   ],
   "source": [
    "print(len(recovered_train_list), recovered_train_list[0])\n",
    "print(len(recovered_train_labels), recovered_train_labels[:10])\n",
    "\n",
    "with open(heinz_data_path+'/'+heinz_data_id+'.recovered_heinz.train', 'w') as f:\n",
    "     f.write(dt.generate_data_text(recovered_train_list))\n",
    "with open(heinz_data_path+'/'+heinz_data_id+'.recovered_heinz.labels', 'w') as f:\n",
    "     f.write(dt.generate_label_text(recovered_train_labels, categorical=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "654acfcd1eb87a4693af85bf6fbf387a0947087ac12d1ae940a37e95173af4d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
